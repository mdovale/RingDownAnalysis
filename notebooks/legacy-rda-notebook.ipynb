{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ring-Down Data Analysis\n",
    "\n",
    "This notebook analyzes real ring-down measurement data using two frequency estimation methods:\n",
    "\n",
    "1. **Nonlinear Least Squares (NLS)** with ring-down model\n",
    "\n",
    "2. **DFT Peak Fitting with Lorentzian Function**\n",
    "\n",
    "The data files are located in the `data/` folder and include both CSV and MAT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import least_squares, curve_fit\n",
    "from scipy.signal import detrend\n",
    "from scipy.signal.windows import kaiser\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Apply consistent plotting style\n",
    "from ringdownanalysis import plots\n",
    "plots.apply_plotting_style()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Estimation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lorentzian_func(f: np.ndarray, A: float, f0: float, gamma: float, offset: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Lorentzian function for power spectrum fitting.\n",
    "    \n",
    "    P(f) = A / ((f - f0)^2 + (gamma/2)^2) + offset\n",
    "    \"\"\"\n",
    "    return A / ((f - f0)**2 + (gamma / 2.0)**2) + offset\n",
    "\n",
    "\n",
    "def _estimate_gamma_from_half_max(P: np.ndarray, k: int, fs: float, N_dft: int) -> float:\n",
    "    \"\"\"\n",
    "    Estimate gamma (FWHM) from half-maximum points in power spectrum.\n",
    "    \n",
    "    Returns:\n",
    "        Estimated gamma value in Hz\n",
    "    \"\"\"\n",
    "    P_max = P[k]\n",
    "    half_max = P_max / 2.0\n",
    "    \n",
    "    # Find left half-maximum point\n",
    "    left_idx = k\n",
    "    for i in range(k - 1, max(0, k - 10), -1):\n",
    "        if P[i] < half_max:\n",
    "            left_idx = i\n",
    "            break\n",
    "    \n",
    "    # Find right half-maximum point\n",
    "    right_idx = k\n",
    "    for i in range(k + 1, min(len(P), k + 10)):\n",
    "        if P[i] < half_max:\n",
    "            right_idx = i\n",
    "            break\n",
    "    \n",
    "    if right_idx > left_idx:\n",
    "        return (right_idx - left_idx) * fs / N_dft\n",
    "    else:\n",
    "        return 2.0 * fs / N_dft\n",
    "\n",
    "\n",
    "def _fit_lorentzian_to_peak(\n",
    "    P: np.ndarray,\n",
    "    k: int,\n",
    "    fs: float,\n",
    "    N_dft: int,\n",
    "    n_points: int = 7,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Fit a Lorentzian function to the power spectrum around the peak.\n",
    "    \n",
    "    For ring-down signals, the Fourier transform has a Lorentzian shape,\n",
    "    so fitting a Lorentzian is more appropriate than, e.g., parabolic interpolation.\n",
    "    \"\"\"\n",
    "    # Determine range of bins to use\n",
    "    half_range = n_points // 2\n",
    "    k_start = max(0, k - half_range)\n",
    "    k_end = min(len(P), k + half_range + 1)\n",
    "    \n",
    "    # Extract frequency and power values\n",
    "    k_indices = np.arange(k_start, k_end)\n",
    "    f_bins = k_indices * fs / N_dft\n",
    "    P_bins = P[k_indices]\n",
    "    \n",
    "    # Initial parameter guesses\n",
    "    P_max = P[k]\n",
    "    f0_init = k * fs / N_dft\n",
    "    gamma_init = _estimate_gamma_from_half_max(P, k, fs, N_dft)\n",
    "    \n",
    "    # Estimate background offset from edges\n",
    "    offset_init = np.min([P[0], P[-1], np.mean(P[:max(1, len(P)//20)])])\n",
    "    \n",
    "    # Initial amplitude guess\n",
    "    A_init = P_max * (gamma_init / 2.0)**2\n",
    "    \n",
    "    # Fit Lorentzian\n",
    "    try:\n",
    "        popt, _ = curve_fit(\n",
    "            _lorentzian_func,\n",
    "            f_bins,\n",
    "            P_bins,\n",
    "            p0=[A_init, f0_init, gamma_init, offset_init],\n",
    "            bounds=([\n",
    "                0.0,\n",
    "                f_bins[0],\n",
    "                0.0,\n",
    "                -np.inf,\n",
    "            ], [\n",
    "                np.inf,\n",
    "                f_bins[-1],\n",
    "                (f_bins[-1] - f_bins[0]) * 2.0,\n",
    "                np.inf,\n",
    "            ]),\n",
    "            maxfev=1000,\n",
    "        )\n",
    "        \n",
    "        A_fit, f0_fit, gamma_fit, offset_fit = popt\n",
    "        \n",
    "        # Calculate delta (offset from bin k)\n",
    "        delta = (f0_fit - f0_init) / (fs / N_dft)\n",
    "        \n",
    "        # Clip delta to reasonable range\n",
    "        delta = np.clip(delta, -0.5, 0.5)\n",
    "        \n",
    "        return delta\n",
    "    except (RuntimeError, ValueError, np.linalg.LinAlgError):\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def _estimate_initial_tau_from_envelope(x: np.ndarray, t: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Estimate initial tau from signal envelope decay using RMS in windows.\n",
    "    \n",
    "    Returns:\n",
    "        Initial tau estimate in seconds\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    window_size = min(1000, N // 10)\n",
    "    n_windows = N // window_size\n",
    "    \n",
    "    rms_values = []\n",
    "    for i in range(n_windows):\n",
    "        start = i * window_size\n",
    "        end = start + window_size\n",
    "        rms_values.append(np.std(x[start:end]))\n",
    "    \n",
    "    rms_values = np.array(rms_values)\n",
    "    rms_peak = np.max(rms_values)\n",
    "    decay_idx = np.where(rms_values < rms_peak * np.exp(-1))[0]\n",
    "    \n",
    "    if len(decay_idx) > 0 and decay_idx[0] > 0:\n",
    "        return t[decay_idx[0] * window_size]\n",
    "    else:\n",
    "        return t[-1] / 2.0\n",
    "\n",
    "\n",
    "def _estimate_initial_parameters_from_dft(x: np.ndarray, fs: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Estimate initial frequency, phase, amplitude, and DC offset from DFT.\n",
    "    \n",
    "    Returns:\n",
    "        (f0_init, phi0_init, A0_init, c0)\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    X = np.fft.rfft(x * np.hanning(N))\n",
    "    mag2 = np.abs(X) ** 2\n",
    "    \n",
    "    # Skip DC component (k=0) when finding peak\n",
    "    mag2_no_dc = mag2.copy()\n",
    "    mag2_no_dc[0] = 0.0\n",
    "    k = int(np.argmax(mag2_no_dc))\n",
    "    \n",
    "    # Use Lorentzian fitting for initial guess if possible\n",
    "    if k > 0 and k < len(mag2) - 1:\n",
    "        delta = _fit_lorentzian_to_peak(mag2, k, fs, N, n_points=7)\n",
    "        k_interp = k + delta\n",
    "    else:\n",
    "        k_interp = k\n",
    "    \n",
    "    f0_init = k_interp * fs / N\n",
    "    phi0_init = np.angle(X[k])\n",
    "    \n",
    "    # Initial amplitude estimation with sanity check\n",
    "    A0_init = np.sqrt(2.0) * np.sqrt(mag2[k] / N)\n",
    "    if A0_init < 0.1 * np.std(x) or A0_init > 10 * np.std(x):\n",
    "        A0_init = np.std(x) * np.sqrt(2.0)\n",
    "    \n",
    "    c0 = np.mean(x)\n",
    "    \n",
    "    return f0_init, phi0_init, A0_init, c0\n",
    "\n",
    "\n",
    "def estimate_freq_nls_ringdown(x: np.ndarray, fs: float, tau_known: float = None) -> float:\n",
    "    \"\"\"\n",
    "    Estimate frequency using nonlinear least squares with ring-down model.\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    t = np.arange(N) / fs\n",
    "    \n",
    "    # Get initial parameter estimates\n",
    "    f0_init, phi0_init, A0_init, c0 = _estimate_initial_parameters_from_dft(x, fs)\n",
    "    \n",
    "    if tau_known is not None:\n",
    "        # Known tau: estimate (A0, f, phi, c)\n",
    "        def residuals(p):\n",
    "            A0, f, phi, c = p\n",
    "            return (A0 * np.exp(-t / tau_known) * np.cos(2.0 * np.pi * f * t + phi) + c) - x\n",
    "        \n",
    "        # Tighter frequency bounds\n",
    "        df = fs / N\n",
    "        f_low = max(0.0, f0_init - max(0.2 * f0_init, 2 * df))\n",
    "        f_high = min(0.5 * fs, f0_init + max(0.2 * f0_init, 2 * df))\n",
    "        \n",
    "        lb = [0.0, f_low, -np.pi, -np.inf]\n",
    "        ub = [10.0 * A0_init, f_high, np.pi, np.inf]\n",
    "        \n",
    "        res = least_squares(\n",
    "            residuals,\n",
    "            x0=np.array([A0_init, f0_init, phi0_init, c0]),\n",
    "            bounds=(lb, ub),\n",
    "            method=\"trf\",\n",
    "            ftol=1e-8,\n",
    "            xtol=1e-8,\n",
    "            gtol=1e-8,\n",
    "            max_nfev=500,\n",
    "            verbose=0,\n",
    "        )\n",
    "        \n",
    "        if not res.success:\n",
    "            return f0_init\n",
    "        \n",
    "        _, f_hat, _, _ = res.x\n",
    "    else:\n",
    "        # Unknown tau: estimate (A0, f, phi, tau, c)\n",
    "        tau_init = _estimate_initial_tau_from_envelope(x, t)\n",
    "        \n",
    "        def residuals(p):\n",
    "            A0, f, phi, tau, c = p\n",
    "            return (A0 * np.exp(-t / tau) * np.cos(2.0 * np.pi * f * t + phi) + c) - x\n",
    "        \n",
    "        df = fs / N\n",
    "        f_low = max(0.0, f0_init - max(0.2 * f0_init, 2 * df))\n",
    "        f_high = min(0.5 * fs, f0_init + max(0.2 * f0_init, 2 * df))\n",
    "        \n",
    "        lb = [0.0, f_low, -np.pi, t[1], -np.inf]\n",
    "        ub = [10.0 * A0_init, f_high, np.pi, 10.0 * t[-1], np.inf]\n",
    "        \n",
    "        res = least_squares(\n",
    "            residuals,\n",
    "            x0=np.array([A0_init, f0_init, phi0_init, tau_init, c0]),\n",
    "            bounds=(lb, ub),\n",
    "            method=\"trf\",\n",
    "            ftol=1e-8,\n",
    "            xtol=1e-8,\n",
    "            gtol=1e-8,\n",
    "            max_nfev=1000,\n",
    "            verbose=0,\n",
    "        )\n",
    "        \n",
    "        if not res.success:\n",
    "            return f0_init\n",
    "        \n",
    "        _, f_hat, _, _, _ = res.x\n",
    "    \n",
    "    # Sanity check\n",
    "    if f_hat < 0 or f_hat > 0.5 * fs or abs(f_hat - f0_init) > 0.5 * f0_init:\n",
    "        return f0_init\n",
    "    \n",
    "    return float(f_hat)\n",
    "\n",
    "\n",
    "def estimate_freq_dft(x: np.ndarray, fs: float, window: str = \"kaiser\", kaiser_beta: float = 9.0) -> float:\n",
    "    \"\"\"\n",
    "    Estimate frequency using DFT peak fitting with Lorentzian function.\n",
    "    Default window is Kaiser with beta=9.0 for high side-lobe suppression.\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    \n",
    "    # Apply window\n",
    "    if window == \"kaiser\":\n",
    "        w = kaiser(N, kaiser_beta)\n",
    "    elif window == \"hann\":\n",
    "        w = np.hanning(N)\n",
    "    elif window == \"rect\":\n",
    "        w = np.ones(N)\n",
    "    elif window == \"blackman\":\n",
    "        w = np.blackman(N)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown window: {window}\")\n",
    "    \n",
    "    xw = x * w\n",
    "    \n",
    "    # Compute one-sided DFT\n",
    "    X = np.fft.rfft(xw)\n",
    "    P = np.abs(X) ** 2\n",
    "    \n",
    "    # Find peak bin (skip DC component k=0)\n",
    "    P_no_dc = P.copy()\n",
    "    P_no_dc[0] = 0.0\n",
    "    k = int(np.argmax(P_no_dc))\n",
    "    \n",
    "    # Guard against edges\n",
    "    if k <= 0 or k >= len(P) - 1:\n",
    "        return k * fs / N\n",
    "    \n",
    "    # Fit Lorentzian to power spectrum around peak\n",
    "    delta = _fit_lorentzian_to_peak(P, k, fs, N, n_points=7)\n",
    "    \n",
    "    f_hat = (k + delta) * fs / N\n",
    "    return float(f_hat)\n",
    "\n",
    "\n",
    "def crlb_var_f_ringdown_explicit(A0: float, sigma: float, fs: float, N: int, tau: float) -> float:\n",
    "    \"\"\"\n",
    "    Cramér-Rao lower bound for frequency estimation variance with ring-down,\n",
    "    calculated from explicit Fisher information matrix.\n",
    "    \"\"\"\n",
    "    Ts = 1.0 / fs\n",
    "    t = np.arange(N) * Ts\n",
    "    \n",
    "    exp_factor = np.exp(-2.0 * t / tau)\n",
    "    S_0 = np.sum(exp_factor)\n",
    "    S_1 = np.sum(t * exp_factor)\n",
    "    S_2 = np.sum(t**2 * exp_factor)\n",
    "    \n",
    "    I_eff_omega = (A0**2 / sigma**2) * (S_2 - S_1**2 / S_0)\n",
    "    \n",
    "    if I_eff_omega < 1e-30:\n",
    "        return np.inf\n",
    "    \n",
    "    crlb_var_f = 1.0 / ((2.0 * np.pi) ** 2 * I_eff_omega)\n",
    "    \n",
    "    return crlb_var_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions for Data Processing\n",
    "\n",
    "def estimate_tau_from_full_data(data: np.ndarray, t: np.ndarray, fs: float) -> float:\n",
    "    \"\"\"\n",
    "    Estimate tau from full data using NLS fit.\n",
    "    \n",
    "    Returns:\n",
    "        Estimated tau value in seconds\n",
    "    \"\"\"\n",
    "    N = len(data)\n",
    "    t_norm = t - t[0]\n",
    "    \n",
    "    # Get initial parameter estimates\n",
    "    f0_init, phi0_init, A0_init, c0 = _estimate_initial_parameters_from_dft(data, fs)\n",
    "    \n",
    "    # Initial tau guess from envelope decay\n",
    "    tau_init = _estimate_initial_tau_from_envelope(data, t_norm)\n",
    "    \n",
    "    # NLS fit to estimate tau: fit (A0, f, phi, tau, c)\n",
    "    def residuals_tau(p):\n",
    "        A0, f, phi, tau, c = p\n",
    "        return (A0 * np.exp(-t_norm / tau) * np.cos(2.0 * np.pi * f * t_norm + phi) + c) - data\n",
    "    \n",
    "    df = fs / N\n",
    "    f_low = max(0.0, f0_init - max(0.2 * f0_init, 2 * df))\n",
    "    f_high = min(0.5 * fs, f0_init + max(0.2 * f0_init, 2 * df))\n",
    "    \n",
    "    lb = [0.0, f_low, -np.pi, t_norm[1], -np.inf]\n",
    "    ub = [10.0 * A0_init, f_high, np.pi, 10.0 * t_norm[-1], np.inf]\n",
    "    \n",
    "    res_tau = least_squares(\n",
    "        residuals_tau,\n",
    "        x0=np.array([A0_init, f0_init, phi0_init, tau_init, c0]),\n",
    "        bounds=(lb, ub),\n",
    "        method=\"trf\",\n",
    "        ftol=1e-8,\n",
    "        xtol=1e-8,\n",
    "        gtol=1e-8,\n",
    "        max_nfev=1000,\n",
    "        verbose=0,\n",
    "    )\n",
    "    \n",
    "    if res_tau.success:\n",
    "        _, _, _, tau_est, _ = res_tau.x\n",
    "        # Sanity check\n",
    "        if tau_est <= 0 or not np.isfinite(tau_est) or tau_est > 10.0 * t_norm[-1] or tau_est < t_norm[1]:\n",
    "            return tau_init\n",
    "        return tau_est\n",
    "    else:\n",
    "        return tau_init\n",
    "\n",
    "\n",
    "def estimate_noise_parameters(data_cropped: np.ndarray, t_crop: np.ndarray, tau_est: float, fs: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Estimate A0 (initial amplitude) and sigma (noise std) from cropped data.\n",
    "    \n",
    "    Returns:\n",
    "        (A0_est, sigma_est)\n",
    "    \"\"\"\n",
    "    N_crop = len(data_cropped)\n",
    "    t_crop_norm = t_crop - t_crop[0]\n",
    "    \n",
    "    # Initial estimate from first portion\n",
    "    n_init = min(1000, N_crop // 10)\n",
    "    A0_est = np.sqrt(2.0) * np.std(data_cropped[:n_init])\n",
    "    \n",
    "    # Fit model to get residuals for noise estimation\n",
    "    def model_residuals(p):\n",
    "        A0, f, phi, c = p\n",
    "        return (A0 * np.exp(-t_crop_norm / tau_est) * np.cos(2.0 * np.pi * f * t_crop_norm + phi) + c) - data_cropped\n",
    "    \n",
    "    # Get initial guesses\n",
    "    f0_init, phi0_init, A0_init, c0 = _estimate_initial_parameters_from_dft(data_cropped, fs)\n",
    "    \n",
    "    # Quick fit to get residuals\n",
    "    df = fs / N_crop\n",
    "    f_low = max(0.0, f0_init - max(0.2 * f0_init, 2 * df))\n",
    "    f_high = min(0.5 * fs, f0_init + max(0.2 * f0_init, 2 * df))\n",
    "    \n",
    "    res_fit = least_squares(\n",
    "        model_residuals,\n",
    "        x0=np.array([A0_init, f0_init, phi0_init, c0]),\n",
    "        bounds=([0.0, f_low, -np.pi, -np.inf], [10.0 * A0_init, f_high, np.pi, np.inf]),\n",
    "        method=\"trf\",\n",
    "        ftol=1e-6,\n",
    "        max_nfev=200,\n",
    "        verbose=0,\n",
    "    )\n",
    "    \n",
    "    if res_fit.success:\n",
    "        residuals = res_fit.fun\n",
    "        sigma_est = np.std(residuals)\n",
    "        A0_est = res_fit.x[0]\n",
    "    else:\n",
    "        # Fallback: estimate noise from tail\n",
    "        tail_start = max(int(0.8 * len(data_cropped)), len(data_cropped) - 1000)\n",
    "        sigma_est = np.std(data_cropped[tail_start:])\n",
    "    \n",
    "    return A0_est, sigma_est\n",
    "\n",
    "\n",
    "def crop_data_to_tau(t: np.ndarray, data: np.ndarray, tau_est: float, min_samples: int = 100) -> tuple:\n",
    "    \"\"\"\n",
    "    Crop data to 3*tau_est to avoid long noisy tail affecting frequency estimation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    t : np.ndarray\n",
    "        Time array\n",
    "    data : np.ndarray\n",
    "        Signal array\n",
    "    tau_est : float\n",
    "        Estimated tau value in seconds\n",
    "    min_samples : int\n",
    "        Minimum number of samples required. If cropped data is shorter, return original.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    (t_crop, data_cropped) - cropped time and signal arrays\n",
    "    \"\"\"\n",
    "    t_crop_max = 3.0 * tau_est\n",
    "    crop_idx = t <= t_crop_max\n",
    "    t_crop = t[crop_idx]\n",
    "    data_cropped = data[crop_idx]\n",
    "    \n",
    "    # If cropped data is too short, return original\n",
    "    if len(t_crop) < min_samples:\n",
    "        return t.copy(), data.copy()\n",
    "    \n",
    "    return t_crop, data_cropped\n",
    "\n",
    "\n",
    "def process_data_file(filepath: str, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Process a single data file (CSV or MAT) and return analysis results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the data file\n",
    "    file_type : str\n",
    "        'CSV' or 'MAT'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Results dictionary with all analysis data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    if file_type == 'CSV':\n",
    "        t, data = load_csv_data(filepath)\n",
    "        V2 = None\n",
    "    else:  # MAT\n",
    "        t, data, V2 = load_mat_data(filepath)\n",
    "    \n",
    "    # Calculate sampling frequency\n",
    "    fs = 1.0 / np.mean(np.diff(t))\n",
    "    \n",
    "    # Estimate tau from full data\n",
    "    tau_est = estimate_tau_from_full_data(data, t, fs)\n",
    "    \n",
    "    # Crop data to 3*tau_est to avoid long noisy tail affecting frequency estimation\n",
    "    t_crop, data_cropped = crop_data_to_tau(t, data, tau_est, min_samples=1000)\n",
    "    \n",
    "    # Warn if cropped data is too short for reliable frequency estimation\n",
    "    # (crop_data_to_tau already handles very short data by returning original)\n",
    "    min_samples_for_analysis = 1000  # Reasonable minimum for frequency estimation\n",
    "    if len(t_crop) < min_samples_for_analysis:\n",
    "        print(f\"  Warning: Cropped data too short ({len(t_crop)} samples < {min_samples_for_analysis}), using original\")\n",
    "        t_crop = t\n",
    "        data_cropped = data\n",
    "    \n",
    "    # Estimate frequencies on cropped data\n",
    "    f_nls = estimate_freq_nls_ringdown(data_cropped, fs)\n",
    "    f_dft = estimate_freq_dft(data_cropped, fs, window='kaiser')\n",
    "    \n",
    "    # Estimate noise parameters\n",
    "    A0_est, sigma_est = estimate_noise_parameters(data_cropped, t_crop, tau_est, fs)\n",
    "    \n",
    "    # Calculate CRLB\n",
    "    N_crop = len(data_cropped)\n",
    "    crlb_var_f = crlb_var_f_ringdown_explicit(A0_est, sigma_est, fs, N_crop, tau_est)\n",
    "    crlb_std_f = np.sqrt(crlb_var_f) if np.isfinite(crlb_var_f) else np.inf\n",
    "    \n",
    "    return {\n",
    "        'filename': Path(filepath).name,\n",
    "        'type': file_type,\n",
    "        't': t,\n",
    "        'data': data,\n",
    "        'V2': V2,\n",
    "        't_crop': t_crop,\n",
    "        'data_cropped': data_cropped,\n",
    "        'fs': fs,\n",
    "        'tau_est': tau_est,\n",
    "        'f_nls': f_nls,\n",
    "        'f_dft': f_dft,\n",
    "        'A0_est': A0_est,\n",
    "        'sigma_est': sigma_est,\n",
    "        'crlb_std_f': crlb_std_f,\n",
    "        'N': len(t),\n",
    "        'N_crop': len(t_crop),\n",
    "        'T': t[-1],\n",
    "        'T_crop': t_crop[-1] if len(t_crop) > 0 else 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We need to:\n",
    "\n",
    "- For CSV files: Load column 1 (time) and column 4 (phase in cycles)\n",
    "\n",
    "- For MAT files: Load `moku.data[:, 1]` (time) and `moku.data[:, 4]` (phase in cycles)\n",
    "\n",
    "- Detrend the phase data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_data_line(line: str) -> bool:\n",
    "    \"\"\"Check if a line contains numeric data (not a header or comment).\"\"\"\n",
    "    if not line or line.startswith('%'):\n",
    "        return False\n",
    "    \n",
    "    parts = line.split(',')\n",
    "    if len(parts) < 4:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        float(parts[0])  # First column should be numeric\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_csv_data(filepath):\n",
    "    \"\"\"\n",
    "    Load CSV data file from Moku:Lab Phasemeter.\n",
    "    \n",
    "    Returns:\n",
    "        t: time array (s), starting from 0\n",
    "        data: phase in cycles (detrended)\n",
    "    \"\"\"\n",
    "    # Read the file, skipping comment lines and headers\n",
    "    data_lines = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if _is_data_line(line):\n",
    "                data_lines.append(line)\n",
    "    \n",
    "    # Parse data\n",
    "    data_list = []\n",
    "    for line in data_lines:\n",
    "        parts = [float(x.strip()) for x in line.split(',')]\n",
    "        data_list.append(parts)\n",
    "    \n",
    "    data_array = np.array(data_list)\n",
    "    \n",
    "    # Extract time (column 1, index 0) and phase (column 4, index 3)\n",
    "    t_raw = data_array[:, 0]\n",
    "    data_raw = data_array[:, 3]  # Column 4 is index 3\n",
    "    \n",
    "    # Time starts from 0\n",
    "    t = t_raw - t_raw[0]\n",
    "    \n",
    "    # Detrend phase data\n",
    "    data = detrend(data_raw, type='constant')\n",
    "    \n",
    "    return t, data\n",
    "\n",
    "\n",
    "def load_mat_data(filepath):\n",
    "    \"\"\"\n",
    "    Load MAT data file from Moku:Lab Phasemeter.\n",
    "    \n",
    "    Returns:\n",
    "        t: time array (s), starting from 0\n",
    "        data: phase in cycles (detrended)\n",
    "        V2: phase in cycles (detrended) or None if not available\n",
    "    \"\"\"\n",
    "    mat_data = loadmat(filepath)\n",
    "    \n",
    "    # Access the moku.data structure\n",
    "    moku_data = mat_data['moku']['data'][0, 0]\n",
    "    \n",
    "    # Extract time (column 1, index 0) and phase (column 4, index 3)\n",
    "    t_raw = moku_data[:, 0].flatten()\n",
    "    data_raw = moku_data[:, 3].flatten()  # Column 4 is index 3\n",
    "    \n",
    "    # Check if V2 exists (column 9, index 8)\n",
    "    V2 = None\n",
    "    if moku_data.shape[1] > 8:\n",
    "        V2_raw = moku_data[:, 8].flatten()  # Column 9 is index 8\n",
    "        V2 = detrend(V2_raw, type='constant')\n",
    "    \n",
    "    # Time starts from 0\n",
    "    t = t_raw - t_raw[0]\n",
    "    \n",
    "    # Detrend phase data\n",
    "    data = detrend(data_raw, type='constant')\n",
    "    \n",
    "    return t, data, V2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find All Data Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data')\n",
    "csv_files = sorted(glob.glob(str(data_dir / '*.csv')))\n",
    "mat_files = sorted(glob.glob(str(data_dir / '*.mat')))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files and {len(mat_files)} MAT files\")\n",
    "print(\"\\nCSV files:\")\n",
    "for f in csv_files:\n",
    "    print(f\"  - {Path(f).name}\")\n",
    "print(\"\\nMAT files:\")\n",
    "for f in mat_files:\n",
    "    print(f\"  - {Path(f).name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Each Data File\n",
    "\n",
    "For each file, we'll:\n",
    "\n",
    "1. Load the data\n",
    "\n",
    "2. Calculate sampling frequency\n",
    "\n",
    "3. Estimate tau from envelope decay\n",
    "\n",
    "4. Crop data to $3 \\cdot \\tau$ to avoid long noisy tail affecting frequency estimation\n",
    "\n",
    "5. Estimate frequency using both NLS and DFT methods on cropped data\n",
    "\n",
    "6. Estimate A0 (initial amplitude) and sigma (noise std) for CRLB calculation\n",
    "\n",
    "7. Calculate the CRLB (Cramér-Rao Lower Bound) for frequency estimation\n",
    "\n",
    "8. Store results for comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Process CSV files (use tau_known=True)\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        print(f\"\\nProcessing {Path(csv_file).name}...\")\n",
    "        r = process_data_file(csv_file, 'CSV')\n",
    "        results.append(r)\n",
    "        \n",
    "        print(f\"  Sampling frequency: {r['fs']:.2f} Hz\")\n",
    "        print(f\"  Estimated tau: {r['tau_est']:.2f} s\")\n",
    "        print(f\"  Cropped to: {r['T_crop']:.2f} s ({r['N_crop']} samples, {r['N_crop']/r['N']*100:.1f}% of original)\")\n",
    "        print(f\"  NLS frequency: {r['f_nls']:.6f} Hz\")\n",
    "        print(f\"  DFT frequency: {r['f_dft']:.6f} Hz\")\n",
    "        print(f\"  Difference: {abs(r['f_nls'] - r['f_dft']):.6e} Hz\")\n",
    "        print(f\"  CRLB std: {r['crlb_std_f']:.6e} Hz\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {Path(csv_file).name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Process MAT files (use tau_known=False)\n",
    "for mat_file in mat_files:\n",
    "    try:\n",
    "        print(f\"\\nProcessing {Path(mat_file).name}...\")\n",
    "        r = process_data_file(mat_file, 'MAT')\n",
    "        # Add fs_crop field for MAT files (for backward compatibility)\n",
    "        r['fs_crop'] = r['fs']\n",
    "        results.append(r)\n",
    "        \n",
    "        print(f\"  Sampling frequency: {r['fs']:.2f} Hz\")\n",
    "        print(f\"  Estimated tau: {r['tau_est']:.2f} s\")\n",
    "        print(f\"  Cropped to: {r['T_crop']:.2f} s ({r['N_crop']} samples, {r['N_crop']/r['N']*100:.1f}% of original)\")\n",
    "        print(f\"  NLS frequency: {r['f_nls']:.6f} Hz\")\n",
    "        print(f\"  DFT frequency: {r['f_dft']:.6f} Hz\")\n",
    "        print(f\"  Difference: {abs(r['f_nls'] - r['f_dft']):.6e} Hz\")\n",
    "        print(f\"  CRLB std: {r['crlb_std_f']:.6e} Hz\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {Path(mat_file).name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n\\nSuccessfully processed {len(results)} files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table\n",
    "\n",
    "Create a summary table comparing the results from both methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_data = []\n",
    "for r in results:\n",
    "    summary_data.append({\n",
    "        'Filename': r['filename'],\n",
    "        'Type': r['type'],\n",
    "        'N (samples)': r['N'],\n",
    "        'N_crop (samples)': r['N_crop'],\n",
    "        'T (s)': f\"{r['T']:.2f}\",\n",
    "        'T_crop (s)': f\"{r['T_crop']:.2f}\",\n",
    "        'fs (Hz)': f\"{r['fs']:.2f}\",\n",
    "        'tau_est (s)': f\"{r['tau_est']:.2f}\",\n",
    "        'f_NLS (Hz)': f\"{r['f_nls']:.6f}\",\n",
    "        'f_DFT (Hz)': f\"{r['f_dft']:.6f}\",\n",
    "        '|f_NLS - f_DFT| (Hz)': f\"{abs(r['f_nls'] - r['f_dft']):.6e}\",\n",
    "        'CRLB std (Hz)': f\"{r['crlb_std_f']:.6e}\",\n",
    "        'A0_est': f\"{r['A0_est']:.4f}\",\n",
    "        'sigma_est': f\"{r['sigma_est']:.6e}\",\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"Summary of Frequency Estimation Results:\")\n",
    "print(\"=\" * 120)\n",
    "print(df_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Plot the time series data and frequency estimates for each file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all results\n",
    "n_files = len(results)\n",
    "n_cols = 2\n",
    "n_rows = (n_files + 1) // 2\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
    "if n_files == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for idx, r in enumerate(results):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot time series (original)\n",
    "    t_plot = r['t']\n",
    "    data_plot = r['data']\n",
    "    \n",
    "    # Downsample for plotting if too many points\n",
    "    if len(t_plot) > 100000:\n",
    "        step = len(t_plot) // 100000\n",
    "        t_plot = t_plot[::step]\n",
    "        data_plot = data_plot[::step]\n",
    "    \n",
    "    ax.plot(t_plot, data_plot, 'b-', alpha=0.3, linewidth=0.5, label='Original data')\n",
    "    \n",
    "    # Plot cropped data (highlighted)\n",
    "    t_crop_plot = r['t_crop']\n",
    "    data_cropped_plot = r['data_cropped']\n",
    "    \n",
    "    if len(t_crop_plot) > 100000:\n",
    "        step = len(t_crop_plot) // 100000\n",
    "        t_crop_plot = t_crop_plot[::step]\n",
    "        data_cropped_plot = data_cropped_plot[::step]\n",
    "    \n",
    "    ax.plot(t_crop_plot, data_cropped_plot, 'r-', alpha=0.7, linewidth=0.8, label='Cropped data (used)')\n",
    "    \n",
    "    # Add vertical line at 3*tau_est\n",
    "    t_crop_max = 3.0 * r['tau_est']\n",
    "    ax.axvline(t_crop_max, color='g', linestyle='--', linewidth=2, alpha=0.7, label=f'3×τ = {t_crop_max:.1f} s')\n",
    "    \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Phase (cycles, detrended)')\n",
    "    ax.set_title(f\"{r['filename']}\\nNLS: {r['f_nls']:.6f} Hz, DFT: {r['f_dft']:.6f} Hz\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "    # Add frequency estimates and CRLB as text\n",
    "    textstr = (f\"f_NLS = {r['f_nls']:.6f} Hz\\n\"\n",
    "               f\"f_DFT = {r['f_dft']:.6f} Hz\\n\"\n",
    "               f\"Δf = {abs(r['f_nls'] - r['f_dft']):.2e} Hz\\n\"\n",
    "               f\"CRLB std = {r['crlb_std_f']:.2e} Hz\")\n",
    "    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "            fontsize=8)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_files, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Factor Analysis\n",
    "\n",
    "Calculate and plot the quality factor $Q$ for each measurement, where $Q = \\pi f_0 \\tau$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q factor for each result\n",
    "# Q = pi * f * tau (from tau = Q/(pi*f))\n",
    "for r in results:\n",
    "    # Use NLS frequency estimate for Q calculation\n",
    "    r['Q'] = np.pi * r['f_nls'] * r['tau_est']\n",
    "\n",
    "# Plot Q factor across all measurements\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Extract Q values and filenames\n",
    "Q_values = [r['Q'] for r in results]\n",
    "filenames_short = [Path(r['filename']).stem[:30] for r in results]\n",
    "\n",
    "# Plot 1: Q factor bar chart\n",
    "ax = axes[0]\n",
    "x_pos = np.arange(len(Q_values))\n",
    "bars = ax.bar(x_pos, Q_values, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('File Index')\n",
    "ax.set_ylabel('Quality Factor Q')\n",
    "ax.set_title('Quality Factor Q Across Measurements')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'{i}' for i in range(len(Q_values))], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, Q) in enumerate(zip(bars, Q_values)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{Q:.1e}',\n",
    "            ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "# Plot 2: Q factor scatter with statistics\n",
    "ax = axes[1]\n",
    "ax.scatter(x_pos, Q_values, s=100, alpha=0.6, edgecolors='black')\n",
    "Q_mean = np.mean(Q_values)\n",
    "Q_std = np.std(Q_values)\n",
    "ax.axhline(Q_mean, color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Mean = {Q_mean:.2e}')\n",
    "ax.fill_between(x_pos, Q_mean - Q_std, Q_mean + Q_std, \n",
    "                alpha=0.2, color='r', label=f'±1 std = {Q_std:.2e}')\n",
    "ax.set_xlabel('File Index')\n",
    "ax.set_ylabel('Quality Factor Q')\n",
    "ax.set_title('Quality Factor Q with Statistics')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'{i}' for i in range(len(Q_values))], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nQ Factor Statistics:\")\n",
    "print(f\"  Mean Q: {Q_mean:.2e}\")\n",
    "print(f\"  Std Q: {Q_std:.2e}\")\n",
    "print(f\"  Min Q: {np.min(Q_values):.2e}\")\n",
    "print(f\"  Max Q: {np.max(Q_values):.2e}\")\n",
    "print(f\"  Range: {np.max(Q_values) - np.min(Q_values):.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Plot\n",
    "\n",
    "Compare the frequency estimates from both methods across all files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Extract frequencies\n",
    "f_nls_all = [r['f_nls'] for r in results]\n",
    "f_dft_all = [r['f_dft'] for r in results]\n",
    "filenames_short = [Path(r['filename']).stem[:30] for r in results]\n",
    "\n",
    "# Plot 1: Scatter comparison\n",
    "ax = axes[0]\n",
    "ax.scatter(f_nls_all, f_dft_all, s=100, alpha=0.6, edgecolors='black')\n",
    "# Add diagonal line\n",
    "f_min = min(min(f_nls_all), min(f_dft_all))\n",
    "f_max = max(max(f_nls_all), max(f_dft_all))\n",
    "ax.plot([f_min, f_max], [f_min, f_max], 'r--', label='Perfect agreement')\n",
    "ax.set_xlabel('NLS Frequency (Hz)')\n",
    "ax.set_ylabel('DFT Frequency (Hz)')\n",
    "ax.set_title('Frequency Estimation Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Difference histogram\n",
    "ax = axes[1]\n",
    "diffs = [abs(f_nls - f_dft) for f_nls, f_dft in zip(f_nls_all, f_dft_all)]\n",
    "ax.bar(range(len(diffs)), diffs, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('File Index')\n",
    "ax.set_ylabel('|f_NLS - f_DFT| (Hz)')\n",
    "ax.set_title('Frequency Difference Between Methods')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMean absolute difference: {np.mean(diffs):.6e} Hz\")\n",
    "print(f\"Max absolute difference: {np.max(diffs):.6e} Hz\")\n",
    "print(f\"Min absolute difference: {np.min(diffs):.6e} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRLB Analysis\n",
    "\n",
    "Compare the frequency estimation differences with the theoretical CRLB (Cramér-Rao Lower Bound).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Extract data\n",
    "diffs = [abs(r['f_nls'] - r['f_dft']) for r in results]\n",
    "crlb_stds = [r['crlb_std_f'] for r in results]\n",
    "filenames_short = [Path(r['filename']).stem[:30] for r in results]\n",
    "\n",
    "# Plot 1: Frequency difference vs CRLB\n",
    "ax = axes[0]\n",
    "ax.scatter(crlb_stds, diffs, s=100, alpha=0.6, edgecolors='black')\n",
    "# Add diagonal line (1:1 ratio)\n",
    "max_val = max(max(crlb_stds), max(diffs))\n",
    "min_val = min(min(crlb_stds), min(diffs))\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='1:1 ratio', linewidth=2)\n",
    "ax.set_xlabel('CRLB std (Hz)')\n",
    "ax.set_ylabel('|f_NLS - f_DFT| (Hz)')\n",
    "ax.set_title('Frequency Difference vs CRLB')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Ratio of difference to CRLB\n",
    "ax = axes[1]\n",
    "ratios = [d / crlb if crlb > 0 and np.isfinite(crlb) else np.nan \n",
    "          for d, crlb in zip(diffs, crlb_stds)]\n",
    "valid_ratios = [r for r in ratios if not np.isnan(r)]\n",
    "ax.bar(range(len(ratios)), ratios, alpha=0.7, edgecolor='black')\n",
    "ax.axhline(1.0, color='r', linestyle='--', linewidth=2, label='CRLB (theoretical limit)')\n",
    "ax.set_xlabel('File Index')\n",
    "ax.set_ylabel('|f_NLS - f_DFT| / CRLB std')\n",
    "ax.set_title('Frequency Difference Relative to CRLB')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCRLB Statistics:\")\n",
    "print(f\"  Mean CRLB std: {np.mean([c for c in crlb_stds if np.isfinite(c)]):.6e} Hz\")\n",
    "print(f\"  Min CRLB std: {np.min([c for c in crlb_stds if np.isfinite(c)]):.6e} Hz\")\n",
    "print(f\"  Max CRLB std: {np.max([c for c in crlb_stds if np.isfinite(c)]):.6e} Hz\")\n",
    "print(f\"\\nRatio Statistics (|Δf| / CRLB):\")\n",
    "print(f\"  Mean ratio: {np.mean(valid_ratios):.4f}\")\n",
    "print(f\"  Median ratio: {np.median(valid_ratios):.4f}\")\n",
    "print(f\"  Max ratio: {np.max(valid_ratios):.4f}\")\n",
    "print(f\"  Min ratio: {np.min(valid_ratios):.4f}\")\n",
    "print(f\"\\nNote: Ratio < 1 means the difference is smaller than the theoretical limit (good!)\")\n",
    "print(f\"      Ratio > 1 means the difference exceeds the theoretical limit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first file for detailed analysis\n",
    "if len(results) > 0:\n",
    "    r = results[0]  # Change index to analyze different files\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    t = r['t']\n",
    "    data = r['data']\n",
    "    t_crop = r['t_crop']\n",
    "    data_cropped = r['data_cropped']\n",
    "    fs = r['fs']\n",
    "    \n",
    "    # Plot 1: Time series (original and cropped)\n",
    "    ax = axes[0, 0]\n",
    "    # Downsample for plotting\n",
    "    step = max(1, len(t) // 50000)\n",
    "    ax.plot(t[::step], data[::step], 'b-', alpha=0.3, linewidth=0.5, label='Original')\n",
    "    step_crop = max(1, len(t_crop) // 50000)\n",
    "    ax.plot(t_crop[::step_crop], data_cropped[::step_crop], 'r-', alpha=0.7, linewidth=0.8, label='Cropped (used)')\n",
    "    # Add vertical line at 3*tau\n",
    "    t_crop_max = 3.0 * r['tau_est']\n",
    "    ax.axvline(t_crop_max, color='g', linestyle='--', linewidth=2, alpha=0.7, label=f'3×τ = {t_crop_max:.1f} s')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Phase (cycles, detrended)')\n",
    "    ax.set_title(f\"Time Series: {r['filename']}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Envelope (cropped data)\n",
    "    ax = axes[0, 1]\n",
    "    from scipy.signal import hilbert\n",
    "    analytic_signal = hilbert(data_cropped)\n",
    "    envelope = np.abs(analytic_signal)\n",
    "    ax.plot(t_crop[::step_crop], envelope[::step_crop], 'g-', alpha=0.7, linewidth=1)\n",
    "    ax.axvline(t_crop_max, color='g', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Envelope (cycles)')\n",
    "    ax.set_title('Signal Envelope (Cropped Data)')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Power Spectral Density (using cropped data)\n",
    "    ax = axes[1, 0]\n",
    "    # Compute PSD using Welch's method\n",
    "    from scipy.signal import welch\n",
    "    f_psd, Pxx = welch(data_cropped, fs, nperseg=min(8192, len(data_cropped)//4))\n",
    "    ax.semilogy(f_psd, Pxx, 'b-', linewidth=1)\n",
    "    ax.axvline(r['f_nls'], color='r', linestyle='--', linewidth=2, label=f\"NLS: {r['f_nls']:.6f} Hz\")\n",
    "    ax.axvline(r['f_dft'], color='g', linestyle='--', linewidth=2, label=f\"DFT: {r['f_dft']:.6f} Hz\")\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('Power Spectral Density')\n",
    "    ax.set_title('Power Spectral Density (Cropped Data)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Zoom around estimated frequencies\n",
    "    f_center = (r['f_nls'] + r['f_dft']) / 2\n",
    "    f_range = max(abs(r['f_nls'] - r['f_dft']) * 10, 10.0)\n",
    "    ax.set_xlim([max(0, f_center - f_range), f_center + f_range])\n",
    "    \n",
    "    # Plot 4: Zoomed PSD around peak\n",
    "    ax = axes[1, 1]\n",
    "    ax.semilogy(f_psd, Pxx, 'b-', linewidth=1)\n",
    "    ax.axvline(r['f_nls'], color='r', linestyle='--', linewidth=2, label=f\"NLS: {r['f_nls']:.6f} Hz\")\n",
    "    ax.axvline(r['f_dft'], color='g', linestyle='--', linewidth=2, label=f\"DFT: {r['f_dft']:.6f} Hz\")\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('Power Spectral Density')\n",
    "    ax.set_title('PSD (Zoomed)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Zoom\n",
    "    f_range = max(abs(r['f_nls'] - r['f_dft']) * 10, 1.0)\n",
    "    ax.set_xlim([max(0, f_center - f_range), f_center + f_range])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nDetailed Analysis for: {r['filename']}\")\n",
    "    print(f\"  Sampling frequency: {fs:.6f} Hz\")\n",
    "    print(f\"  Original number of samples: {r['N']}\")\n",
    "    print(f\"  Cropped number of samples: {r['N_crop']} ({r['N_crop']/r['N']*100:.1f}% of original)\")\n",
    "    print(f\"  Original duration: {r['T']:.2f} s\")\n",
    "    print(f\"  Cropped duration: {r['T_crop']:.2f} s\")\n",
    "    print(f\"  Estimated tau: {r['tau_est']:.2f} s\")\n",
    "    print(f\"  Crop limit (3×τ): {3.0*r['tau_est']:.2f} s\")\n",
    "    print(f\"  NLS frequency: {r['f_nls']:.12f} Hz\")\n",
    "    print(f\"  DFT frequency: {r['f_dft']:.12f} Hz\")\n",
    "    print(f\"  Difference: {abs(r['f_nls'] - r['f_dft']):.12e} Hz\")\n",
    "    print(f\"  Relative difference: {abs(r['f_nls'] - r['f_dft']) / r['f_nls'] * 1e6:.4f} ppm\")\n",
    "    print(f\"  CRLB std: {r['crlb_std_f']:.12e} Hz\")\n",
    "    print(f\"  A0 estimate: {r['A0_est']:.6f}\")\n",
    "    print(f\"  Sigma (noise) estimate: {r['sigma_est']:.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency Analysis Across Realizations\n",
    "\n",
    "In addition to comparing NLS vs DFT and both against CRLB, we also want to assess the consistency of each method across different experimental realizations (different measurement files). This helps us understand:\n",
    "\n",
    "1. **NLS consistency**: How much do NLS frequency estimates vary across different realizations?\n",
    "\n",
    "2. **DFT consistency**: How much do DFT frequency estimates vary across different realizations?\n",
    "\n",
    "We'll compute pairwise differences between all realizations for each method and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frequencies for all realizations\n",
    "f_nls_all = np.array([r['f_nls'] for r in results])\n",
    "f_dft_all = np.array([r['f_dft'] for r in results])\n",
    "filenames = [r['filename'] for r in results]\n",
    "n_realizations = len(results)\n",
    "\n",
    "# Compute pairwise differences for NLS\n",
    "nls_pairwise_diffs = []\n",
    "nls_pairwise_indices = []\n",
    "for i in range(n_realizations):\n",
    "    for j in range(i + 1, n_realizations):\n",
    "        diff = abs(f_nls_all[i] - f_nls_all[j])\n",
    "        nls_pairwise_diffs.append(diff)\n",
    "        nls_pairwise_indices.append((i, j))\n",
    "\n",
    "# Compute pairwise differences for DFT\n",
    "dft_pairwise_diffs = []\n",
    "dft_pairwise_indices = []\n",
    "for i in range(n_realizations):\n",
    "    for j in range(i + 1, n_realizations):\n",
    "        diff = abs(f_dft_all[i] - f_dft_all[j])\n",
    "        dft_pairwise_diffs.append(diff)\n",
    "        dft_pairwise_indices.append((i, j))\n",
    "\n",
    "nls_pairwise_diffs = np.array(nls_pairwise_diffs)\n",
    "dft_pairwise_diffs = np.array(dft_pairwise_diffs)\n",
    "\n",
    "print(f\"Number of realizations: {n_realizations}\")\n",
    "print(f\"Number of pairwise comparisons: {len(nls_pairwise_diffs)}\")\n",
    "print(f\"\\nNLS pairwise differences:\")\n",
    "print(f\"  Mean: {np.mean(nls_pairwise_diffs):.6e} Hz\")\n",
    "print(f\"  Median: {np.median(nls_pairwise_diffs):.6e} Hz\")\n",
    "print(f\"  Std: {np.std(nls_pairwise_diffs):.6e} Hz\")\n",
    "print(f\"  Min: {np.min(nls_pairwise_diffs):.6e} Hz\")\n",
    "print(f\"  Max: {np.max(nls_pairwise_diffs):.6e} Hz\")\n",
    "print(f\"\\nDFT pairwise differences:\")\n",
    "print(f\"  Mean: {np.mean(dft_pairwise_diffs):.6e} Hz\")\n",
    "print(f\"  Median: {np.median(dft_pairwise_diffs):.6e} Hz\")\n",
    "print(f\"  Std: {np.std(dft_pairwise_diffs):.6e} Hz\")\n",
    "print(f\"  Min: {np.min(dft_pairwise_diffs):.6e} Hz\")\n",
    "print(f\"  Max: {np.max(dft_pairwise_diffs):.6e} Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pairwise differences\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Histogram of NLS pairwise differences\n",
    "ax = axes[0, 0]\n",
    "ax.hist(nls_pairwise_diffs, bins=20, alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('|f_NLS[i] - f_NLS[j]| (Hz)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('NLS: Pairwise Differences Across Realizations')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axvline(np.mean(nls_pairwise_diffs), color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Mean = {np.mean(nls_pairwise_diffs):.2e} Hz')\n",
    "ax.axvline(np.median(nls_pairwise_diffs), color='g', linestyle='--', linewidth=2,\n",
    "           label=f'Median = {np.median(nls_pairwise_diffs):.2e} Hz')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Histogram of DFT pairwise differences\n",
    "ax = axes[0, 1]\n",
    "ax.hist(dft_pairwise_diffs, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "ax.set_xlabel('|f_DFT[i] - f_DFT[j]| (Hz)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('DFT: Pairwise Differences Across Realizations')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axvline(np.mean(dft_pairwise_diffs), color='r', linestyle='--', linewidth=2,\n",
    "           label=f'Mean = {np.mean(dft_pairwise_diffs):.2e} Hz')\n",
    "ax.axvline(np.median(dft_pairwise_diffs), color='g', linestyle='--', linewidth=2,\n",
    "           label=f'Median = {np.median(dft_pairwise_diffs):.2e} Hz')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Comparison of NLS vs DFT pairwise differences\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(nls_pairwise_diffs, dft_pairwise_diffs, alpha=0.6, s=50, edgecolors='black')\n",
    "# Add diagonal line\n",
    "max_val = max(np.max(nls_pairwise_diffs), np.max(dft_pairwise_diffs))\n",
    "min_val = min(np.min(nls_pairwise_diffs), np.min(dft_pairwise_diffs))\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='1:1 ratio')\n",
    "ax.set_xlabel('NLS pairwise difference (Hz)')\n",
    "ax.set_ylabel('DFT pairwise difference (Hz)')\n",
    "ax.set_title('NLS vs DFT: Pairwise Differences Comparison')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Box plot comparison\n",
    "ax = axes[1, 1]\n",
    "data = [nls_pairwise_diffs, dft_pairwise_diffs]\n",
    "bp = ax.boxplot(data, labels=['NLS', 'DFT'], patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "    patch.set_alpha(0.7)\n",
    "ax.set_ylabel('Pairwise difference (Hz)')\n",
    "ax.set_title('Pairwise Differences: NLS vs DFT')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix visualization of pairwise differences\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# NLS pairwise difference matrix\n",
    "ax = axes[0]\n",
    "nls_matrix = np.zeros((n_realizations, n_realizations))\n",
    "for idx, (i, j) in enumerate(nls_pairwise_indices):\n",
    "    diff = nls_pairwise_diffs[idx]\n",
    "    nls_matrix[i, j] = diff\n",
    "    nls_matrix[j, i] = diff  # Make symmetric\n",
    "\n",
    "im1 = ax.imshow(nls_matrix, cmap='viridis', aspect='auto', origin='upper')\n",
    "ax.set_title('NLS: Pairwise Frequency Differences (Hz)')\n",
    "ax.set_xlabel('Realization Index')\n",
    "ax.set_ylabel('Realization Index')\n",
    "plt.colorbar(im1, ax=ax, label='|f_NLS[i] - f_NLS[j]| (Hz)')\n",
    "\n",
    "# Add text annotations for small matrices\n",
    "if n_realizations <= 15:\n",
    "    for i in range(n_realizations):\n",
    "        for j in range(n_realizations):\n",
    "            if i != j:\n",
    "                text = ax.text(j, i, f'{nls_matrix[i, j]:.2e}',\n",
    "                             ha=\"center\", va=\"center\", color=\"white\", fontsize=7)\n",
    "\n",
    "# DFT pairwise difference matrix\n",
    "ax = axes[1]\n",
    "dft_matrix = np.zeros((n_realizations, n_realizations))\n",
    "for idx, (i, j) in enumerate(dft_pairwise_indices):\n",
    "    diff = dft_pairwise_diffs[idx]\n",
    "    dft_matrix[i, j] = diff\n",
    "    dft_matrix[j, i] = diff  # Make symmetric\n",
    "\n",
    "im2 = ax.imshow(dft_matrix, cmap='viridis', aspect='auto', origin='upper')\n",
    "ax.set_title('DFT: Pairwise Frequency Differences (Hz)')\n",
    "ax.set_xlabel('Realization Index')\n",
    "ax.set_ylabel('Realization Index')\n",
    "plt.colorbar(im2, ax=ax, label='|f_DFT[i] - f_DFT[j]| (Hz)')\n",
    "\n",
    "# Add text annotations for small matrices\n",
    "if n_realizations <= 15:\n",
    "    for i in range(n_realizations):\n",
    "        for j in range(n_realizations):\n",
    "            if i != j:\n",
    "                text = ax.text(j, i, f'{dft_matrix[i, j]:.2e}',\n",
    "                             ha=\"center\", va=\"center\", color=\"white\", fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall statistics for consistency\n",
    "# Compute standard deviation across all realizations for each method\n",
    "nls_std_across_realizations = np.std(f_nls_all)\n",
    "dft_std_across_realizations = np.std(f_dft_all)\n",
    "\n",
    "# Compute mean frequency for each method\n",
    "nls_mean = np.mean(f_nls_all)\n",
    "dft_mean = np.mean(f_dft_all)\n",
    "\n",
    "# Compute coefficient of variation (relative standard deviation)\n",
    "nls_cv = nls_std_across_realizations / nls_mean if nls_mean > 0 else np.inf\n",
    "dft_cv = dft_std_across_realizations / dft_mean if dft_mean > 0 else np.inf\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Consistency Analysis Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nNLS Method:\")\n",
    "print(f\"  Mean frequency across realizations: {nls_mean:.9f} Hz\")\n",
    "print(f\"  Std across realizations: {nls_std_across_realizations:.6e} Hz\")\n",
    "print(f\"  Coefficient of variation: {nls_cv:.2e}\")\n",
    "print(f\"  Range: [{np.min(f_nls_all):.9f}, {np.max(f_nls_all):.9f}] Hz\")\n",
    "print(f\"  Span: {np.max(f_nls_all) - np.min(f_nls_all):.6e} Hz\")\n",
    "\n",
    "print(f\"\\nDFT Method:\")\n",
    "print(f\"  Mean frequency across realizations: {dft_mean:.9f} Hz\")\n",
    "print(f\"  Std across realizations: {dft_std_across_realizations:.6e} Hz\")\n",
    "print(f\"  Coefficient of variation: {dft_cv:.2e}\")\n",
    "print(f\"  Range: [{np.min(f_dft_all):.9f}, {np.max(f_dft_all):.9f}] Hz\")\n",
    "print(f\"  Span: {np.max(f_dft_all) - np.min(f_dft_all):.6e} Hz\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  NLS span / DFT span: {(np.max(f_nls_all) - np.min(f_nls_all)) / (np.max(f_dft_all) - np.min(f_dft_all)):.4f}\")\n",
    "print(f\"  NLS std / DFT std: {nls_std_across_realizations / dft_std_across_realizations:.4f}\")\n",
    "\n",
    "# Compare with CRLB\n",
    "mean_crlb = np.mean([r['crlb_std_f'] for r in results if np.isfinite(r['crlb_std_f'])])\n",
    "print(f\"\\nCRLB reference:\")\n",
    "print(f\"  Mean CRLB std: {mean_crlb:.6e} Hz\")\n",
    "print(f\"  NLS std / mean CRLB: {nls_std_across_realizations / mean_crlb:.2f}\")\n",
    "print(f\"  DFT std / mean CRLB: {dft_std_across_realizations / mean_crlb:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frequency estimates across realizations\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: NLS frequencies across realizations\n",
    "ax = axes[0]\n",
    "x_pos = np.arange(n_realizations)\n",
    "ax.scatter(x_pos, f_nls_all, s=100, alpha=0.7, edgecolors='black', label='NLS estimates')\n",
    "ax.axhline(nls_mean, color='r', linestyle='--', linewidth=2, label=f'Mean = {nls_mean:.9f} Hz')\n",
    "ax.fill_between(x_pos, nls_mean - nls_std_across_realizations, \n",
    "                nls_mean + nls_std_across_realizations, \n",
    "                alpha=0.2, color='r', label=f'±1 std = {nls_std_across_realizations:.2e} Hz')\n",
    "ax.set_xlabel('Realization Index')\n",
    "ax.set_ylabel('Frequency (Hz)')\n",
    "ax.set_title('NLS Frequency Estimates Across Realizations')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'{i}' for i in range(n_realizations)], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: DFT frequencies across realizations\n",
    "ax = axes[1]\n",
    "ax.scatter(x_pos, f_dft_all, s=100, alpha=0.7, edgecolors='black', color='orange', label='DFT estimates')\n",
    "ax.axhline(dft_mean, color='r', linestyle='--', linewidth=2, label=f'Mean = {dft_mean:.9f} Hz')\n",
    "ax.fill_between(x_pos, dft_mean - dft_std_across_realizations, \n",
    "                dft_mean + dft_std_across_realizations, \n",
    "                alpha=0.2, color='r', label=f'±1 std = {dft_std_across_realizations:.2e} Hz')\n",
    "ax.set_xlabel('Realization Index')\n",
    "ax.set_ylabel('Frequency (Hz)')\n",
    "ax.set_title('DFT Frequency Estimates Across Realizations')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'{i}' for i in range(n_realizations)], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table showing all frequency estimates\n",
    "import pandas as pd\n",
    "\n",
    "consistency_data = []\n",
    "for i, r in enumerate(results):\n",
    "    consistency_data.append({\n",
    "        'Index': i,\n",
    "        'Filename': Path(r['filename']).name[:40],\n",
    "        'f_NLS (Hz)': f\"{r['f_nls']:.9f}\",\n",
    "        'f_DFT (Hz)': f\"{r['f_dft']:.9f}\",\n",
    "        'Deviation from NLS mean (Hz)': f\"{(r['f_nls'] - nls_mean):.6e}\",\n",
    "        'Deviation from DFT mean (Hz)': f\"{(r['f_dft'] - dft_mean):.6e}\",\n",
    "        'CRLB std (Hz)': f\"{r['crlb_std_f']:.6e}\",\n",
    "    })\n",
    "\n",
    "df_consistency = pd.DataFrame(consistency_data)\n",
    "print(\"Frequency Estimates and Deviations from Mean:\")\n",
    "print(\"=\" * 120)\n",
    "print(df_consistency.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
